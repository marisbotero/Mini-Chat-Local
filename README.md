# ğŸ‹ Mini Chat Local con Docker Model Runner

Un proyecto sÃºper simple para ejecutar un modelo de IA localmente usando Docker Model Runner y una interfaz web minimalista.

## ğŸš€ Inicio RÃ¡pido

### Paso 0: Verifica que tienes Docker instalado

**Docker Model Runner** no se instala por separado. Viene incluido dentro de las imÃ¡genes Docker del namespace `ai/`.

Solo necesitas tener **Docker Desktop** (o Docker Engine) instalado en tu computadora.

**Â¿No tienes Docker?** DescÃ¡rgalo desde: https://www.docker.com/products/docker-desktop

---

### Paso 1: Descarga y ejecuta Ollama con Docker

Abre tu terminal y ejecuta este comando (puedes copiar-pegar sin pensar):

```bash
docker run -d --rm -p 11434:11434 --name ollama-server ollama/ollama
```

**Â¿QuÃ© hace esto?**
- **Descarga automÃ¡ticamente** Ollama (servidor de modelos de IA)
- Levanta un servidor local en el puerto 11434
- Crea una API compatible con OpenAI

**ğŸ’¡ Tip:** 
- La primera vez puede tardar unos minutos (descarga la imagen)
- El contenedor corre en segundo plano (flag `-d`)

### Paso 1.5: Descarga un modelo ligero

Ahora descarga un modelo pequeÃ±o (esto puede tardar unos minutos la primera vez):

```bash
docker exec ollama-server ollama pull phi3:mini
```

**Â¿QuÃ© modelo es?** `phi3:mini` es un modelo pequeÃ±o y rÃ¡pido de Microsoft (~3.8GB), perfecto para empezar.

---

### Paso 2: Instala las dependencias de Python

En otra terminal, navega a esta carpeta y ejecuta:

```bash
pip install -r requirements.txt
```

O instala manualmente:

```bash
pip install openai streamlit python-dotenv
```

---

### Paso 3: Configura las variables de entorno

Crea un archivo llamado `.env` en esta carpeta con el siguiente contenido:

```
OPENAI_API_KEY=docker
OPENAI_BASE_URL=http://localhost:11434/v1
```

**ğŸ’¡ Tip:** Puedes crear el archivo manualmente o copiar estos valores. No necesitas cambiar nada, pero si quieres usar otro puerto o clave, edÃ­talo aquÃ­.

---

### Paso 4: Ejecuta tu interfaz

```bash
streamlit run app.py
```

Se abrirÃ¡ automÃ¡ticamente en tu navegador (normalmente en `http://localhost:8501`).

---

## ğŸ’» Ejecutar desde Visual Studio Code

Si quieres ejecutar el proyecto desde VS Code, es muy simple:

1. **Abre el proyecto en VS Code**: `File` â†’ `Open Folder` â†’ Selecciona la carpeta `app`

2. **Abre la terminal integrada**: Presiona `` Ctrl+` `` (Ctrl + backtick) o ve a `Terminal` â†’ `New Terminal`

3. **Sigue los pasos del README**: Ejecuta los mismos comandos que estÃ¡n arriba, pero desde la terminal de VS Code:
   - Instala dependencias: `pip install -r requirements.txt`
   - Crea el archivo `.env` (haz clic derecho â†’ New File â†’ `.env`)
   - Inicia Docker: `docker run -d --rm -p 11434:11434 --name ollama-server ollama/ollama`
   - Descarga el modelo: `docker exec ollama-server ollama pull phi3:mini`
   - Ejecuta Streamlit: `streamlit run app.py`

Â¡Eso es todo! Los comandos son exactamente los mismos, solo los ejecutas desde la terminal de VS Code.

---

## ğŸ¯ CÃ³mo usar

1. **AsegÃºrate de que Docker estÃ¡ corriendo** (Paso 1)
2. **Abre la interfaz** (Paso 4)
3. **Escribe una pregunta** en el cuadro de texto
4. **Haz clic en "Enviar"**
5. **Â¡Disfruta de las respuestas!** ğŸ‰

---

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ README.md          # Estas instrucciones
â”œâ”€â”€ app.py            # Interfaz Streamlit
â”œâ”€â”€ .env              # Variables de entorno
â”œâ”€â”€ requirements.txt  # Dependencias Python
â””â”€â”€ .gitignore       # Archivos a ignorar en Git
```

---

## ğŸ”§ SoluciÃ³n de Problemas

**"No puedo conectar al servidor"**
- Verifica que Docker estÃ¡ corriendo (Paso 1)
- AsegÃºrate de que el puerto 11434 estÃ¡ libre
- Revisa que `.env` tiene la URL correcta
- Verifica que el contenedor estÃ¡ corriendo: `docker ps`

**"El modelo no responde"**
- Espera unos segundos despuÃ©s de iniciar Docker (el modelo necesita cargarse)
- Verifica en la terminal de Docker que no hay errores

**"Streamlit no se abre"**
- Ejecuta `streamlit run app.py` de nuevo
- Abre manualmente `http://localhost:8501` en tu navegador

---

## ğŸ’¡ PrÃ³ximos Pasos

- Prueba otros modelos del namespace `ai/` cambiando el comando Docker
- Personaliza la interfaz en `app.py`
- Agrega historial de conversaciÃ³n
- Experimenta con diferentes modelos segÃºn tus necesidades

---

**Â¡Disfruta creando con IA local! ğŸš€**

