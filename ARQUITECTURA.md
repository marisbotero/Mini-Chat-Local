# ğŸ—ï¸ Arquitectura del Proyecto

## Diagrama de Arquitectura

```mermaid
graph TB
    subgraph "Usuario"
        U[ğŸ‘¤ Usuario]
        N[Navegador Web]
    end
    
    subgraph "AplicaciÃ³n Python"
        S[Streamlit App<br/>app.py]
        C[Cliente OpenAI<br/>Python SDK]
    end
    
    subgraph "Docker Container"
        O[Ollama Server<br/>Puerto 11434]
        M[Modelo phi3:mini<br/>~3.8GB]
    end
    
    U -->|1. Escribe pregunta| N
    N -->|2. HTTP Request| S
    S -->|3. API Call| C
    C -->|4. HTTP POST<br/>/v1/chat/completions| O
    O -->|5. Procesa con| M
    M -->|6. Genera respuesta| O
    O -->|7. JSON Response| C
    C -->|8. Extrae texto| S
    S -->|9. Muestra respuesta| N
    N -->|10. Ve resultado| U
    
    style U fill:#e1f5ff
    style N fill:#fff4e1
    style S fill:#e8f5e9
    style C fill:#e8f5e9
    style O fill:#f3e5f5
    style M fill:#f3e5f5
```

## ğŸ“Š Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario   â”‚
â”‚  (Navegador)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. Escribe pregunta
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit App     â”‚
â”‚   (app.py)          â”‚
â”‚   Puerto: 8501      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 2. Crea mensaje
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cliente OpenAI     â”‚
â”‚  (Python SDK)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 3. HTTP POST
       â”‚    /v1/chat/completions
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama Server     â”‚
â”‚   (Docker)          â”‚
â”‚   Puerto: 11434     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 4. Procesa con modelo
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   phi3:mini         â”‚
â”‚   (Modelo de IA)    â”‚
â”‚   ~3.8GB            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 5. Genera respuesta
       â”‚
       â”‚ (flujo inverso)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario   â”‚
â”‚  Ve respuestaâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”Œ Componentes Principales

### 1. **Frontend (Streamlit)**
- **Archivo:** `app.py`
- **Puerto:** 8501
- **FunciÃ³n:** Interfaz web simple para el usuario
- **TecnologÃ­a:** Streamlit (Python)

### 2. **Cliente API**
- **LibrerÃ­a:** `openai` (Python SDK)
- **FunciÃ³n:** ComunicaciÃ³n con Ollama usando API compatible con OpenAI
- **Endpoint:** `http://localhost:11434/v1`

### 3. **Servidor de Modelos (Ollama)**
- **Contenedor:** Docker (`ollama/ollama`)
- **Puerto:** 11434
- **FunciÃ³n:** Servidor que ejecuta modelos de IA localmente
- **API:** Compatible con OpenAI API

### 4. **Modelo de IA**
- **Nombre:** `phi3:mini`
- **TamaÃ±o:** ~3.8GB
- **FunciÃ³n:** Genera respuestas de texto basadas en preguntas
- **UbicaciÃ³n:** Dentro del contenedor Docker

## ğŸ“¡ Protocolos y Puertos

| Componente | Puerto | Protocolo | DescripciÃ³n |
|------------|--------|-----------|-------------|
| Streamlit | 8501 | HTTP | Interfaz web |
| Ollama | 11434 | HTTP/REST | API de modelos |

## ğŸ”„ Flujo Completo Paso a Paso

1. **Usuario escribe pregunta** en el navegador (Streamlit UI)
2. **Streamlit captura** la pregunta del input
3. **Cliente OpenAI** crea una peticiÃ³n HTTP POST a Ollama
4. **Ollama recibe** la peticiÃ³n y la procesa con el modelo
5. **Modelo phi3:mini** genera una respuesta
6. **Ollama devuelve** la respuesta en formato JSON
7. **Cliente OpenAI** extrae el texto de la respuesta
8. **Streamlit muestra** la respuesta al usuario en la interfaz

## ğŸ³ Docker

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Docker Container           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    Ollama Server         â”‚  â”‚
â”‚  â”‚    Puerto: 11434         â”‚  â”‚
â”‚  â”‚                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  Modelo phi3:mini  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  (~3.8GB)          â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²
         â”‚ HTTP API
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python App     â”‚
â”‚  (app.py)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Dependencias

```
app.py
  â”œâ”€â”€ streamlit (Interfaz web)
  â”œâ”€â”€ openai (Cliente API)
  â””â”€â”€ python-dotenv (Variables de entorno)
        â”‚
        â””â”€â”€ .env (ConfiguraciÃ³n)
              â”œâ”€â”€ OPENAI_API_KEY
              â””â”€â”€ OPENAI_BASE_URL
```

## ğŸ¯ CaracterÃ­sticas Clave

- âœ… **100% Local**: Todo corre en tu computadora
- âœ… **Sin Internet**: No necesita conexiÃ³n despuÃ©s de descargar
- âœ… **API Compatible**: Usa la misma API que OpenAI
- âœ… **Simple**: Solo 3 componentes principales
- âœ… **Ligero**: Modelo pequeÃ±o (~3.8GB)

---

**ğŸ’¡ Nota:** Este diagrama muestra la arquitectura actual del proyecto. Todos los componentes se ejecutan localmente en tu mÃ¡quina.

